{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "\n",
    "summarizer_model.eval()\n",
    "for param in summarizer_model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a81b0a9903de41d44ee65592838912460f3773114e1576...</td>\n",
       "      <td>One of Norway's most popular studies, paramedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c09f07e9eea49dd8cdf056227c0a36c36110bb8544edd4...</td>\n",
       "      <td>I hope that we will soon be finished. I am tir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6639e650a768ce7dce91ce7313a7ff371dadf09fce33eb...</td>\n",
       "      <td>Have you had to wait a long time to get the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f26d3d5b0ced3ce636a4be7c8338ecd6fa614889fd369d...</td>\n",
       "      <td>Footballer Jim Solbakken delivered a bottom-so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db45cc8651bfdeb8d07f531bdab2262ffb55ae4916f107...</td>\n",
       "      <td>(VÃ¥lerenga  Storhamar 52) Veteran Martin RÃ¸yma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>af6dcad6eab7278c50ab4a1841d6deb74dcbf12101489a...</td>\n",
       "      <td>Otherwise, recent figures show that Chinese in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>416fadd418ab8b655f73b9d841c6262c24b1c679757e0b...</td>\n",
       "      <td>(Sverige-Slovenia 11) Emil Forsberg (30) and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>8e173aa2044b2a9eac2716f32f43478b4cdcc3928dfa7e...</td>\n",
       "      <td>BT's architecture critics give their recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>1e1be58590843755b5cd3647989201e8afeb5f38dbf41d...</td>\n",
       "      <td>What is it? King Harald and Queen Sonja were o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>5a17a32a648ca5f85ac450d7e785cae8fa99c95e6f3b16...</td>\n",
       "      <td>(Philadelphia Flyers  Washington Capitals 21) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Article_ID  \\\n",
       "0    a81b0a9903de41d44ee65592838912460f3773114e1576...   \n",
       "1    c09f07e9eea49dd8cdf056227c0a36c36110bb8544edd4...   \n",
       "2    6639e650a768ce7dce91ce7313a7ff371dadf09fce33eb...   \n",
       "3    f26d3d5b0ced3ce636a4be7c8338ecd6fa614889fd369d...   \n",
       "4    db45cc8651bfdeb8d07f531bdab2262ffb55ae4916f107...   \n",
       "..                                                 ...   \n",
       "436  af6dcad6eab7278c50ab4a1841d6deb74dcbf12101489a...   \n",
       "437  416fadd418ab8b655f73b9d841c6262c24b1c679757e0b...   \n",
       "438  8e173aa2044b2a9eac2716f32f43478b4cdcc3928dfa7e...   \n",
       "439  1e1be58590843755b5cd3647989201e8afeb5f38dbf41d...   \n",
       "440  5a17a32a648ca5f85ac450d7e785cae8fa99c95e6f3b16...   \n",
       "\n",
       "                                               Article  \n",
       "0    One of Norway's most popular studies, paramedi...  \n",
       "1    I hope that we will soon be finished. I am tir...  \n",
       "2    Have you had to wait a long time to get the ca...  \n",
       "3    Footballer Jim Solbakken delivered a bottom-so...  \n",
       "4    (VÃ¥lerenga  Storhamar 52) Veteran Martin RÃ¸yma...  \n",
       "..                                                 ...  \n",
       "436  Otherwise, recent figures show that Chinese in...  \n",
       "437  (Sverige-Slovenia 11) Emil Forsberg (30) and B...  \n",
       "438  BT's architecture critics give their recommend...  \n",
       "439  What is it? King Harald and Queen Sonja were o...  \n",
       "440  (Philadelphia Flyers  Washington Capitals 21) ...  \n",
       "\n",
       "[441 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df=pd.read_csv(\"/teamspace/studios/this_studio/Datasets/translated_articles.csv\")\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 441\n",
      "Example:\n",
      "NID: a81b0a9903de41d44ee65592838912460f3773114e1576d0a8b15e387bfdbdcf\n",
      "Body: One of Norway's most popular studies, paramedics, recently came to UiS. But it's not just grades that apply. Thursday was the entrance exam with physical tests.  The dream job is ambulances and paramedics. That's what I want most\", says Ida Reilstad FlesjÃ¥. Among the breathing persons on the trainin\n"
     ]
    }
   ],
   "source": [
    "nid2body = dict(zip(news_df[\"Article_ID\"], news_df[\"Article\"]))\n",
    "\n",
    "# Example check\n",
    "print(f\"Total items: {len(nid2body)}\")\n",
    "sample_nid = list(nid2body.keys())[0]\n",
    "print(f\"Example:\\nNID: {sample_nid}\\nBody: {nid2body[sample_nid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary_id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Source</th>\n",
       "      <th>Article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U_0_S_1</td>\n",
       "      <td>Despite obstacles such as knee injuries, stude...</td>\n",
       "      <td>One of Norway's most popular studies, paramedi...</td>\n",
       "      <td>a81b0a9903de41d44ee65592838912460f3773114e1576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U_0_S_2</td>\n",
       "      <td>Summary: Daniel Vigdel Hansen and Rakel Helghe...</td>\n",
       "      <td>I hope that we will soon be finished. I am tir...</td>\n",
       "      <td>c09f07e9eea49dd8cdf056227c0a36c36110bb8544edd4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U_0_S_3</td>\n",
       "      <td>Never before have so many taken the driver's c...</td>\n",
       "      <td>Have you had to wait a long time to get the ca...</td>\n",
       "      <td>6639e650a768ce7dce91ce7313a7ff371dadf09fce33eb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U_0_S_4</td>\n",
       "      <td>What is it? Footballer Jim Solbakken has seen ...</td>\n",
       "      <td>Football manager Jim Solbakken delivered a sol...</td>\n",
       "      <td>f26d3d5b0ced3ce636a4be7c8338ecd6fa614889fd369d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U_0_S_5</td>\n",
       "      <td>What is it? In the final match between VÃ¥leren...</td>\n",
       "      <td>(VÃ¥lerenga  Storhamar 52) Veteran Martin RÃ¸yma...</td>\n",
       "      <td>db45cc8651bfdeb8d07f531bdab2262ffb55ae4916f107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>U_38_S_176</td>\n",
       "      <td>Solbak  a veterinarian who works at Anicura ve...</td>\n",
       "      <td>But sometimes the hospital doesn't have blood ...</td>\n",
       "      <td>94d75ba68a891e27f00f04e16959c3dce449b4e46f63c2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>U_38_S_177</td>\n",
       "      <td>Professor George Church told the BBC that they...</td>\n",
       "      <td>They will try to splice this DNA with genes fr...</td>\n",
       "      <td>17a29b85dc9bfa1cb2a2d04b324545a26843c707c3996c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>U_38_S_178</td>\n",
       "      <td>The Statistical Institute confirms that the ac...</td>\n",
       "      <td>Despite the decline, the activity in the commo...</td>\n",
       "      <td>e2184d047286618e2e90d0981aba083e042b665fc3f0aa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>U_38_S_179</td>\n",
       "      <td>Reuters published that the Vatican owns more t...</td>\n",
       "      <td>On Saturday, the Vatican published a list of i...</td>\n",
       "      <td>b820fc345a06cd4bbdad7a6922dae6eb1fef8619ee6e35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>U_38_S_180</td>\n",
       "      <td>Several U.S. states use lotteries to make it m...</td>\n",
       "      <td>Several U.S. states are holding lotteries to e...</td>\n",
       "      <td>e8a0a0586806116a71007d38e5dfb6e6a9d2a037bc2471...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1099 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Summary_id                                            Summary  \\\n",
       "0        U_0_S_1  Despite obstacles such as knee injuries, stude...   \n",
       "1        U_0_S_2  Summary: Daniel Vigdel Hansen and Rakel Helghe...   \n",
       "2        U_0_S_3  Never before have so many taken the driver's c...   \n",
       "3        U_0_S_4  What is it? Footballer Jim Solbakken has seen ...   \n",
       "4        U_0_S_5  What is it? In the final match between VÃ¥leren...   \n",
       "...          ...                                                ...   \n",
       "1094  U_38_S_176  Solbak  a veterinarian who works at Anicura ve...   \n",
       "1095  U_38_S_177  Professor George Church told the BBC that they...   \n",
       "1096  U_38_S_178  The Statistical Institute confirms that the ac...   \n",
       "1097  U_38_S_179  Reuters published that the Vatican owns more t...   \n",
       "1098  U_38_S_180  Several U.S. states use lotteries to make it m...   \n",
       "\n",
       "                                                 Source  \\\n",
       "0     One of Norway's most popular studies, paramedi...   \n",
       "1     I hope that we will soon be finished. I am tir...   \n",
       "2     Have you had to wait a long time to get the ca...   \n",
       "3     Football manager Jim Solbakken delivered a sol...   \n",
       "4     (VÃ¥lerenga  Storhamar 52) Veteran Martin RÃ¸yma...   \n",
       "...                                                 ...   \n",
       "1094  But sometimes the hospital doesn't have blood ...   \n",
       "1095  They will try to splice this DNA with genes fr...   \n",
       "1096  Despite the decline, the activity in the commo...   \n",
       "1097  On Saturday, the Vatican published a list of i...   \n",
       "1098  Several U.S. states are holding lotteries to e...   \n",
       "\n",
       "                                             Article_id  \n",
       "0     a81b0a9903de41d44ee65592838912460f3773114e1576...  \n",
       "1     c09f07e9eea49dd8cdf056227c0a36c36110bb8544edd4...  \n",
       "2     6639e650a768ce7dce91ce7313a7ff371dadf09fce33eb...  \n",
       "3     f26d3d5b0ced3ce636a4be7c8338ecd6fa614889fd369d...  \n",
       "4     db45cc8651bfdeb8d07f531bdab2262ffb55ae4916f107...  \n",
       "...                                                 ...  \n",
       "1094  94d75ba68a891e27f00f04e16959c3dce449b4e46f63c2...  \n",
       "1095  17a29b85dc9bfa1cb2a2d04b324545a26843c707c3996c...  \n",
       "1096  e2184d047286618e2e90d0981aba083e042b665fc3f0aa...  \n",
       "1097  b820fc345a06cd4bbdad7a6922dae6eb1fef8619ee6e35...  \n",
       "1098  e8a0a0586806116a71007d38e5dfb6e6a9d2a037bc2471...  \n",
       "\n",
       "[1099 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_df=pd.read_csv(\"/teamspace/studios/this_studio/Datasets/translated_summaries.csv\")\n",
    "sum_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items: 1099\n",
      "Example:\n",
      "SID: U_0_S_1\n",
      "Summary: Despite obstacles such as knee injuries, students like Thea Dyrli Djuve show that the way to qualify for the study can be successful for those who manage to pass the physical tests. Students such as Ida Reilstad FlesjÃ¥ had to give up due to a knee injury, while Thea Dyrli Djuve managed to pass the t\n"
     ]
    }
   ],
   "source": [
    "# # Path to your sid2text pickle file\n",
    "# with open('/teamspace/studios/this_studio/Data/sid2text.pkl', 'rb') as f:\n",
    "#     sid2text = pickle.load(f)\n",
    "# len(sid2text)\n",
    "\n",
    "sid2text = dict(zip(sum_df[\"Summary_id\"], sum_df[\"Summary\"]))\n",
    "\n",
    "# Example check\n",
    "print(f\"Total items: {len(sid2text)}\")\n",
    "sample_sid = list(sid2text.keys())[0]\n",
    "print(f\"Example:\\nSID: {sample_sid}\\nSummary: {sid2text[sample_sid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# === Device and Precision Setup ===\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 768\n",
    "\n",
    "# === Utility Functions ===\n",
    "def KDE_PMI(emb1, emb2, bandwidth=0.7):\n",
    "    bandwidth = torch.tensor(bandwidth, dtype=torch.float32, device=emb1.device)\n",
    "    diff = emb1 - emb2\n",
    "    sim = torch.exp(- (diff ** 2) / (2 * bandwidth ** 2))\n",
    "    return sim\n",
    "\n",
    "def get_embedding(key, table, dim):\n",
    "    if key not in table:\n",
    "        table[key] = torch.nn.Parameter(torch.randn(dim, dtype=torch.float32, device=device) * 0.01, requires_grad=True)\n",
    "    return table[key]\n",
    "\n",
    "# === Load Embeddings ===\n",
    "def load_embeddings(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        if isinstance(data, dict):\n",
    "            return {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in data.items()}\n",
    "        elif isinstance(data, (list, tuple, np.ndarray)):\n",
    "            # Create a dummy dict with index keys if needed\n",
    "            return {str(i): torch.tensor(v, dtype=torch.float32, device=device) for i, v in enumerate(data)}\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported embedding data format in {path}\")\n",
    "\n",
    "summary_embed = load_embeddings(\"/teamspace/studios/this_studio/summary_embeddings.pkl\")\n",
    "newsbody_embed = load_embeddings(\"/teamspace/studios/this_studio/newsbody_embeddings.pkl\")\n",
    "headline_embed = load_embeddings(\"/teamspace/studios/this_studio/headline_embeddings.pkl\")\n",
    "\n",
    "embed_tables = {\n",
    "    'summary': summary_embed,\n",
    "    'newsbody': newsbody_embed,\n",
    "    'headline': headline_embed\n",
    "}\n",
    "\n",
    "# === Load Dataset ===\n",
    "lookup_df = pd.read_csv(\"/teamspace/studios/this_studio/Datasets/Personalsum_Behavior_Vocab.csv\").set_index('EdgeID')\n",
    "\n",
    "# Use safe CSV loading for malformed file\n",
    "train_df = pd.read_csv(\n",
    "    \"/teamspace/studios/this_studio/Datasets/personalsum_train_impersum.csv\",\n",
    "    on_bad_lines='skip',  # Skip rows with malformed quoting\n",
    "    engine='python'       # More fault-tolerant parser\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Tail2Idx from Bhist and Bpos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting tails: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [00:05<00:00, 239.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tail2Idx built: 1540 unique tail IDs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tail_set = set()\n",
    "\n",
    "print(\"Building Tail2Idx from Bhist and Bpos...\")\n",
    "for row in tqdm(train_df.itertuples(), total=len(train_df), desc=\"Collecting tails\"):\n",
    "    try:\n",
    "        bhist = literal_eval(row.Bhist)\n",
    "        bpos = row.Bpos\n",
    "\n",
    "        # Add tails from Bhist\n",
    "        for b_id in bhist:\n",
    "            if b_id in lookup_df.index:\n",
    "                tail = lookup_df.loc[b_id, 'Tail']\n",
    "                tail_set.add(tail)\n",
    "\n",
    "        # Add tail from Bpos (in case it's not already in Bhist)\n",
    "        if bpos in lookup_df.index:\n",
    "            tail = lookup_df.loc[bpos, 'Tail']\n",
    "            tail_set.add(tail)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Skip] Error in row {row.Index}: {e}\")\n",
    "\n",
    "# === Final mappings ===\n",
    "tail_ids = sorted(tail_set)\n",
    "tail2idx = {tid: idx for idx, tid in enumerate(tail_ids)}\n",
    "idx2tail = {idx: tid for tid, idx in tail2idx.items()}\n",
    "\n",
    "print(f\"Tail2Idx built: {len(tail2idx)} unique tail IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_dwell = nn.Linear(1, hidden_dim, bias=False)\n",
    "        self.gamma_rel = nn.Parameter(torch.empty(hidden_dim, dtype=torch.float32, device=device))\n",
    "        self.y_e = nn.Parameter(torch.empty(hidden_dim, dtype=torch.float32, device=device))\n",
    "        self.f = nn.Parameter(torch.empty(1, dtype=torch.float32, device=device))\n",
    "        self.y_s = nn.Parameter(torch.empty(hidden_dim, dtype=torch.float32, device=device))\n",
    "        self.y_focus = nn.Parameter(torch.empty(hidden_dim, dtype=torch.float32, device=device))\n",
    "        self.y_acc = nn.Parameter(torch.empty(hidden_dim, dtype=torch.float32, device=device))\n",
    "        self.K_short = nn.Parameter(torch.randn(30, dtype=torch.float32, device=device))  # last 50 steps\n",
    "        self.K_long  = nn.Parameter(torch.randn(10, dtype=torch.float32, device=device))\n",
    "        self.K_event = nn.Parameter(torch.randn(30, dtype=torch.float32, device=device))\n",
    "\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, len(tail2idx))  # for tail_id classification\n",
    "        self.bpos_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.init_parameters()\n",
    "\n",
    "    def init_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.W_dwell.weight, nonlinearity='linear')\n",
    "        for param in [self.gamma_rel, self.y_e, self.y_s, self.y_focus, self.y_acc]:\n",
    "            nn.init.normal_(param, mean=0.0, std=0.01)\n",
    "        with torch.no_grad():\n",
    "            self.f.copy_(torch.tensor([0.5], dtype=torch.float32, device=device))\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables):\n",
    "        total_loss = torch.tensor(0., dtype=torch.float32, device=device)\n",
    "        enc_loss = torch.tensor(0., dtype=torch.float32, device=device)\n",
    "        pred_loss = torch.tensor(0., dtype=torch.float32, device=device)\n",
    "\n",
    "        prev_e_b = torch.zeros(hidden_dim, dtype=torch.float32, device=device)\n",
    "        memory = []\n",
    "\n",
    "        for t, b_id in enumerate(Bhist):\n",
    "            if b_id not in lookup_df.index:\n",
    "                continue\n",
    "\n",
    "            row = lookup_df.loc[b_id]\n",
    "            head_id, rel, tail_id = row['Head'], row['Relation'], row['Tail']\n",
    "            dwell_s = row['dwell'] / 130000.0 if pd.notna(row['dwell']) else None\n",
    "\n",
    "            head_emb = prev_e_b if t > 0 else embed_tables['headline'].get(head_id, torch.zeros(hidden_dim, device=device))\n",
    "            e_b = torch.zeros(hidden_dim, dtype=torch.float32, device=device)\n",
    "            mem_e_b = torch.zeros(hidden_dim, dtype=torch.float32, device=device)\n",
    "            \n",
    "\n",
    "            if rel in ['click', 'skip', 'gen_summ']:\n",
    "                tail_emb = embed_tables['newsbody'].get(tail_id, torch.zeros(hidden_dim, device=device))\n",
    "                if rel == 'click':\n",
    "                    dwell_v = self.W_dwell(torch.tensor([[dwell_s]], dtype=torch.float32, device=device)).squeeze(0) if dwell_s else torch.zeros(hidden_dim, device=device)\n",
    "                    e_b = tail_emb + (dwell_v * self.y_e * KDE_PMI(tail_emb, head_emb))\n",
    "                elif rel == 'skip':\n",
    "                    e_b = self.f * tail_emb + (1. - self.f) * KDE_PMI(tail_emb, head_emb)\n",
    "                elif rel == 'gen_summ':\n",
    "                    hl_emb = embed_tables['headline'].get(tail_id, torch.zeros(hidden_dim, device=device))\n",
    "                    e_b = self.y_s * KDE_PMI(tail_emb, head_emb) * hl_emb\n",
    "\n",
    "            elif rel == 'summ_gen':\n",
    "                sum_emb = embed_tables['summary'].get(tail_id, torch.zeros(hidden_dim, device=device))\n",
    "                doc_emb = embed_tables['newsbody'].get(head_id, torch.zeros(hidden_dim, device=device))\n",
    "                e_b = self.y_focus * KDE_PMI(sum_emb, head_emb) + self.y_acc * KDE_PMI(sum_emb, doc_emb)\n",
    "            else:\n",
    "                tail_emb = embed_tables['newsbody'].get(tail_id, torch.zeros(hidden_dim, device=device))\n",
    "                e_b = tail_emb\n",
    "\n",
    "            # Apply Î³ to e_b\n",
    "            e_b = self.gamma_rel * e_b\n",
    "            e_b_orig = e_b.clone()\n",
    "            # --- MEMORY INJECTION ---\n",
    "            if memory:\n",
    "                idxs = torch.arange(len(memory)-1, -1, -1, device=device)\n",
    "                memory_stack = torch.stack(memory)\n",
    "\n",
    "                w_short = self.K_short[idxs.clamp(max=len(self.K_short)-1)]\n",
    "                w_long  = self.K_long [idxs.clamp(max=len(self.K_long )-1)]\n",
    "                w_event = self.K_event[idxs.clamp(max=len(self.K_event)-1)]\n",
    "\n",
    "                m_short = (w_short.unsqueeze(1) * memory_stack).sum(0)\n",
    "                m_long  = (w_long .unsqueeze(1) * memory_stack).sum(0)\n",
    "                m_event = (w_event.unsqueeze(1) * memory_stack).sum(0)\n",
    "\n",
    "                gates = torch.softmax(torch.stack([m_short, m_long, m_event]), dim=0)\n",
    "                mem_e_b = gates[0]*m_short + gates[1]*m_long + gates[2]*m_event\n",
    "\n",
    "            memory.append(mem_e_b.detach())  # Detach to stop gradient\n",
    "            e_b = e_b_orig + mem_e_b\n",
    "            prev_e_b = e_b\n",
    "\n",
    "            # BCE or CE loss over current tail_id\n",
    "            if tail_id in tail2idx:\n",
    "                logits = self.classifier(e_b.unsqueeze(0))\n",
    "                target = torch.tensor([tail2idx[tail_id]], dtype=torch.long, device=device)\n",
    "                enc_loss += F.cross_entropy(logits, target)\n",
    "\n",
    "        # === Final Step Prediction ===\n",
    "        e_b_pos_pred = self.bpos_mlp(e_b)\n",
    "        b_next = lookup_df.loc[Bpos]\n",
    "        tpos_id = b_next['Tail']\n",
    "        logits_pos = self.classifier(e_b_pos_pred.unsqueeze(0))\n",
    "        target_pos = torch.tensor([tail2idx[tpos_id]], dtype=torch.long, device=device)\n",
    "        pred_loss = F.cross_entropy(logits_pos, target_pos)\n",
    "\n",
    "        total_loss = (0.5 * enc_loss + 0.5 * pred_loss)/(100*len(Bhist)+1)\n",
    "        return e_b, e_b_pos_pred, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SummaryInverseMapper(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, e_b_pos, e_b_hist, e_doc):\n",
    "        x = torch.cat([e_b_pos, e_b_hist, e_doc], dim=-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class CrossAttentionAdapter(nn.Module):\n",
    "    def __init__(self, query_dim, key_value_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.query_proj = nn.Linear(query_dim, hidden_dim)\n",
    "        self.key_proj = nn.Linear(key_value_dim, hidden_dim)\n",
    "        self.value_proj = nn.Linear(key_value_dim, hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        if query.dim() == 1: query = query.unsqueeze(0)\n",
    "        if key.dim() == 1: key = key.unsqueeze(0)\n",
    "        if value.dim() == 1: value = value.unsqueeze(0)\n",
    "\n",
    "        Q = self.query_proj(query).unsqueeze(1)  # [B, 1, H]\n",
    "        K = self.key_proj(key).unsqueeze(1)      # [B, 1, H]\n",
    "        V = self.value_proj(value).unsqueeze(1)  # [B, 1, H]\n",
    "\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attended = torch.matmul(attn_weights, V)\n",
    "        return self.out_proj(attended.squeeze(1))\n",
    "\n",
    "\n",
    "class B2SModel(nn.Module):\n",
    "    def __init__(self, encoder, inverse_mapper, adapter, summarizer_model, tokenizer, nid2body, sid2text):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.inverse_mapper = inverse_mapper\n",
    "        self.adapter = adapter\n",
    "        self.summarizer = summarizer_model  # T5ForConditionalGeneration\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nid2body = nid2body\n",
    "        self.sid2text = sid2text\n",
    "        self.adapter_to_t5 = nn.Linear(adapter.out_proj.out_features, summarizer_model.config.d_model)\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables):\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Step 1: Encode user behavior\n",
    "        e_b, e_b_pos_pred, loss_enc = self.encoder(Bhist, Bpos, lookup_df, tail2idx, embed_tables)\n",
    "\n",
    "        # Step 2: Get document embedding and text\n",
    "        head_id = lookup_df.loc[Bpos]['Head']\n",
    "        tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "        e_doc = embed_tables['newsbody'].get(head_id, torch.zeros_like(e_b))\n",
    "        doc_text = self.nid2body.get(head_id, \"\")\n",
    "\n",
    "        # Step 3: Predict personalized summary embedding\n",
    "        e_s_pos_pred = self.inverse_mapper(e_b_pos_pred, e_b, e_doc)\n",
    "        adapted = self.adapter(e_s_pos_pred, e_doc, e_doc)\n",
    "        adapted_proj = self.adapter_to_t5(adapted)  # [1, d_model]\n",
    "\n",
    "        # Step 4: Encode document text with T5\n",
    "        input_enc = self.tokenizer(doc_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        encoder_outputs = self.summarizer.encoder(\n",
    "            input_ids=input_enc['input_ids'],\n",
    "            attention_mask=input_enc['attention_mask'],\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Step 5: Prepare teacher-forced decoder inputs\n",
    "        sid = lookup_df.loc[Bpos]['Tail']\n",
    "        gold_summary = self.sid2text.get(sid, \"\")\n",
    "        gold_inputs = self.tokenizer(gold_summary, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        decoder_input_ids = gold_inputs['input_ids'][:, :-1].contiguous()\n",
    "        decoder_labels = gold_inputs['input_ids'][:, 1:].contiguous()\n",
    "\n",
    "        # Step 6: Inject adapter vector as first token\n",
    "        adapter_token_embed = adapted_proj.unsqueeze(1)  # [1, 1, d_model]\n",
    "        dec_embed = self.summarizer.get_input_embeddings()(decoder_input_ids)  # [1, T, d_model]\n",
    "        final_embed = torch.cat([adapter_token_embed, dec_embed], dim=1)\n",
    "\n",
    "        decoder_labels = torch.cat([\n",
    "            torch.full((1, 1), -100, dtype=torch.long, device=device),  # mask adapter loss\n",
    "            decoder_labels\n",
    "        ], dim=1)\n",
    "\n",
    "        # Step 7: Decode with adapter-injected embeddings\n",
    "        outputs = self.summarizer(\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_inputs_embeds=final_embed,\n",
    "            labels=decoder_labels\n",
    "        )\n",
    "        loss_txt = outputs.loss\n",
    "\n",
    "        # Step 8: Decode predicted summary\n",
    "        pred_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "        pred_summary = self.tokenizer.decode(pred_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Step 9: Return losses and summaries\n",
    "        impersum_loss = 0.5 * loss_enc + 0.5 * loss_txt\n",
    "        return (impersum_loss, loss_txt, loss_enc), pred_summary, gold_summary, doc_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Define Supporting Modules ===\n",
    "inverse_mapper = SummaryInverseMapper(hidden_dim).to(device)\n",
    "adapter = CrossAttentionAdapter(query_dim=768, key_value_dim=768, hidden_dim=768).to(device)\n",
    "\n",
    "encoder = BehaviorEncoder().to(device)\n",
    "\n",
    "# === Initialize the Combined B2S Model ===\n",
    "b2s_model = B2SModel(\n",
    "    encoder=encoder,\n",
    "    inverse_mapper=inverse_mapper,\n",
    "    adapter=adapter,\n",
    "    summarizer_model=summarizer_model,\n",
    "    tokenizer=tokenizer,\n",
    "    nid2body=nid2body,\n",
    "    sid2text=sid2text\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>Bhist</th>\n",
       "      <th>Bpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0_1</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4']</td>\n",
       "      <td>B5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0_2</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']</td>\n",
       "      <td>B8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0_3</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...</td>\n",
       "      <td>B12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0_4</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...</td>\n",
       "      <td>B17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0_5</td>\n",
       "      <td>['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...</td>\n",
       "      <td>B20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1248</td>\n",
       "      <td>38_234</td>\n",
       "      <td>['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...</td>\n",
       "      <td>B4844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>1249</td>\n",
       "      <td>38_235</td>\n",
       "      <td>['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...</td>\n",
       "      <td>B4847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1250</td>\n",
       "      <td>38_236</td>\n",
       "      <td>['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...</td>\n",
       "      <td>B4852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1251</td>\n",
       "      <td>38_237</td>\n",
       "      <td>['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...</td>\n",
       "      <td>B4855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>1252</td>\n",
       "      <td>38_238</td>\n",
       "      <td>['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...</td>\n",
       "      <td>B4858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1253 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  UserID                                              Bhist  \\\n",
       "0              0     0_1                           ['B1', 'B2', 'B3', 'B4']   \n",
       "1              1     0_2         ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']   \n",
       "2              2     0_3  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...   \n",
       "3              3     0_4  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...   \n",
       "4              4     0_5  ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8...   \n",
       "...          ...     ...                                                ...   \n",
       "1248        1248  38_234  ['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...   \n",
       "1249        1249  38_235  ['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...   \n",
       "1250        1250  38_236  ['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...   \n",
       "1251        1251  38_237  ['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...   \n",
       "1252        1252  38_238  ['B3965', 'B3966', 'B3967', 'B3968', 'B3969', ...   \n",
       "\n",
       "       Bpos  \n",
       "0        B5  \n",
       "1        B8  \n",
       "2       B12  \n",
       "3       B17  \n",
       "4       B20  \n",
       "...     ...  \n",
       "1248  B4844  \n",
       "1249  B4847  \n",
       "1250  B4852  \n",
       "1251  B4855  \n",
       "1252  B4858  \n",
       "\n",
       "[1253 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 300 | Test samples: 953\n"
     ]
    }
   ],
   "source": [
    "train_rows = train_df.iloc[:300]\n",
    "test_rows = train_df.iloc[300:1253]\n",
    "\n",
    "\n",
    "\n",
    "def extract_pairs(df, lookup_df, tail2idx):\n",
    "    data_pairs = []\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            Bhist = literal_eval(row['Bhist'])\n",
    "            Bpos = row['Bpos']  # explicitly given next behavior\n",
    "            if Bpos in lookup_df.index:\n",
    "                tail_id = lookup_df.loc[Bpos, 'Tail']\n",
    "                if tail_id in tail2idx:\n",
    "                    data_pairs.append((Bhist, Bpos))  # use full Bhist, and correct Bpos\n",
    "        except:\n",
    "            continue\n",
    "    return data_pairs\n",
    "\n",
    "\n",
    "train_data = extract_pairs(train_rows, lookup_df, tail2idx)\n",
    "test_data = extract_pairs(test_rows, lookup_df, tail2idx)\n",
    "\n",
    "print(f\"Train samples: {len(train_data)} | Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running B2S Forward Pass ===\n",
      "Sample Index: 272\n",
      "Behavior History Length: 26\n",
      "Bpos: B1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted e_b_pos shape: torch.Size([768])\n",
      "True Tail ID (Bpos): U_13_S_3\n",
      "\n",
      "--- Personalized Summary Output ---\n",
      "Gold Summary   : Aalesund has struggled with weak results, with only one victory in the last nine matches. The match against Sarpsborg 08 was regarded as one of the worst performances, with players and coaches acknowledging lack of setup and quality. Despite changes and attempts at an excitement, the match ended with a superior victory for Sarpsborg 08, which has turned the trend with two subsequent victories after a long series of losses.\n",
      "Predicted      : sden however   and and in  against Aps s  a as  of the   in with only and coaches ing  of  and.. , , an,,  match against with a superior and in Sarps und 08. which   the  in two wins winss in a.. of..\n",
      "\n",
      "--- Loss Breakdown ---\n",
      "Total Loss     : 2.0122\n",
      "Gen Loss       : 3.9863\n",
      "Encoder Loss   : 0.0381\n"
     ]
    }
   ],
   "source": [
    "# === Pick one sample ===\n",
    "sample_idx = random.randint(0, len(train_data) - 1)\n",
    "Bhist, Bpos = train_data[sample_idx]\n",
    "\n",
    "print(f\"\\n=== Running B2S Forward Pass ===\")\n",
    "print(f\"Sample Index: {sample_idx}\")\n",
    "print(f\"Behavior History Length: {len(Bhist)}\")\n",
    "print(f\"Bpos: {Bpos}\")\n",
    "\n",
    "b2s_model.eval()\n",
    "with torch.no_grad():\n",
    "    e_b, e_b_pos_pred, _ = b2s_model.encoder(Bhist, Bpos, lookup_df, tail2idx, embed_tables)\n",
    "\n",
    "    head_id = lookup_df.loc[Bpos]['Head']\n",
    "    e_doc = embed_tables['newsbody'].get(head_id, torch.zeros_like(e_b))\n",
    "\n",
    "    e_s_pos_pred = b2s_model.inverse_mapper(e_b_pos_pred, e_b, e_doc)\n",
    "\n",
    "    # Run model to get detailed losses\n",
    "    (loss_total, loss_gen, loss_enc), pred_summary, gold_summary, generic_summary = b2s_model(\n",
    "        Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "    )\n",
    "\n",
    "# === Metadata ===\n",
    "true_tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "print(f\"\\nPredicted e_b_pos shape: {e_b_pos_pred.shape}\")\n",
    "print(f\"True Tail ID (Bpos): {true_tail_id}\")\n",
    "\n",
    "# === Summaries ===\n",
    "print(\"\\n--- Personalized Summary Output ---\")\n",
    "print(f\"Gold Summary   : {gold_summary}\")\n",
    "print(f\"Predicted      : {pred_summary}\")\n",
    "\n",
    "# === Losses ===\n",
    "print(\"\\n--- Loss Breakdown ---\")\n",
    "print(f\"Total Loss     : {loss_total:.4f}\")\n",
    "print(f\"Gen Loss       : {loss_gen:.4f}\")\n",
    "print(f\"Encoder Loss   : {loss_enc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¦ Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:43<00:00,  1.83it/s, Loss=0.1192, GenLoss=0.2022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Epoch 1] Avg Loss: 0.8503 | Gen Loss: 1.6631\n",
      "ðŸ“Œ Saved checkpoint: b2s_model_personalsum_epoch001.pt\n",
      "\n",
      "ðŸŸ¦ Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:41<00:00,  1.85it/s, Loss=0.3458, GenLoss=0.6579] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Epoch 2] Avg Loss: 0.2915 | Gen Loss: 0.5491\n",
      "ðŸ“Œ Saved checkpoint: b2s_model_personalsum_epoch002.pt\n",
      "\n",
      "ðŸŸ¦ Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:37<00:00,  1.90it/s, Loss=0.2116, GenLoss=0.3910]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Epoch 3] Avg Loss: 0.2709 | Gen Loss: 0.5094\n",
      "ðŸ“Œ Saved checkpoint: b2s_model_personalsum_epoch003.pt\n",
      "\n",
      "ðŸŸ¦ Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:39<00:00,  1.88it/s, Loss=0.2583, GenLoss=0.4852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Epoch 4] Avg Loss: 0.3016 | Gen Loss: 0.5715\n",
      "ðŸ“Œ Saved checkpoint: b2s_model_personalsum_epoch004.pt\n",
      "\n",
      "ðŸŸ¦ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [02:37<00:00,  1.91it/s, Loss=0.2122, GenLoss=0.3933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Epoch 5] Avg Loss: 0.2658 | Gen Loss: 0.5006\n",
      "ðŸ“Œ Saved checkpoint: b2s_model_personalsum_epoch005.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# === Config ===\n",
    "lr = 1e-3\n",
    "num_epochs = 5\n",
    "save_every = 1\n",
    "checkpoint_template = \"b2s_model_personalsum_epoch{:03d}.pt\"\n",
    "resume = True  # set to False if starting fresh\n",
    "start_epoch = 0\n",
    "\n",
    "# === Optimizer ===\n",
    "optimizer = torch.optim.Adam(b2s_model.parameters(), lr=lr)\n",
    "\n",
    "# === Load Checkpoint if Resume Enabled ===\n",
    "if resume:\n",
    "    checkpoint_files = [f for f in os.listdir() if f.startswith(\"b2s_model_openai_epoch\") and f.endswith(\".pt\")]\n",
    "    if checkpoint_files:\n",
    "        last_ckpt = sorted(checkpoint_files)[-1]\n",
    "        ckpt_epoch = int(last_ckpt.split(\"epoch\")[-1].split(\".pt\")[0])\n",
    "        start_epoch = ckpt_epoch\n",
    "        b2s_model.load_state_dict(torch.load(last_ckpt))\n",
    "        print(f\"ðŸ” Loaded checkpoint '{last_ckpt}' and resumed from epoch {start_epoch + 1}\")\n",
    "\n",
    "# === Training Loop ===\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    b2s_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_gen_loss = 0.0\n",
    "\n",
    "    print(f\"\\nðŸŸ¦ Epoch {epoch + 1}/{num_epochs}\")\n",
    "    pbar = tqdm(train_data, desc=f\"Training Epoch {epoch + 1}\", dynamic_ncols=True)\n",
    "\n",
    "    for Bhist, Bpos in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            (loss_total, loss_txt, loss_enc), pred_summary, gold_summary, _ = b2s_model(\n",
    "                Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "            )\n",
    "        except Exception as e:\n",
    "            pbar.set_postfix_str(f\"[Skip {Bpos}] {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss_total.item()\n",
    "        total_gen_loss += loss_txt.item()\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{loss_total.item():.4f}\",\n",
    "            \"GenLoss\": f\"{loss_txt.item():.4f}\"\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(train_data)\n",
    "    avg_gen_loss = total_gen_loss / len(train_data)\n",
    "    print(f\"âœ… [Epoch {epoch + 1}] Avg Loss: {avg_loss:.4f} | Gen Loss: {avg_gen_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % save_every == 0:\n",
    "        ckpt_path = checkpoint_template.format(epoch + 1)\n",
    "        torch.save(b2s_model.state_dict(), ckpt_path)\n",
    "        print(f\"ðŸ“Œ Saved checkpoint: {ckpt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model checkpoint: b2s_model_personalsum_epoch005.pt\n"
     ]
    }
   ],
   "source": [
    "# === Re-initialize your model architecture (same as training) ===\n",
    "encoder = BehaviorEncoder().to(device)\n",
    "inverse_mapper = SummaryInverseMapper(hidden_dim).to(device)\n",
    "adapter = CrossAttentionAdapter(query_dim=768, key_value_dim=768, hidden_dim=768).to(device)\n",
    "\n",
    "b2s_model = B2SModel(\n",
    "    encoder=encoder,\n",
    "    inverse_mapper=inverse_mapper,\n",
    "    adapter=adapter,\n",
    "    summarizer_model=summarizer_model,\n",
    "    tokenizer=tokenizer,\n",
    "    nid2body=nid2body,\n",
    "    sid2text=sid2text\n",
    ").to(device)\n",
    "\n",
    "# === Load weights ===\n",
    "checkpoint_path = \"b2s_model_personalsum_epoch005.pt\"  # <-- change as needed\n",
    "b2s_model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "print(f\"âœ… Loaded model checkpoint: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2SModel(\n",
       "  (encoder): BehaviorEncoder(\n",
       "    (W_dwell): Linear(in_features=1, out_features=768, bias=False)\n",
       "    (classifier): Linear(in_features=768, out_features=1540, bias=True)\n",
       "    (bpos_mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (inverse_mapper): SummaryInverseMapper(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=2304, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (adapter): CrossAttentionAdapter(\n",
       "    (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (summarizer): T5ForConditionalGeneration(\n",
       "    (shared): Embedding(32128, 1024)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 1024)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 16)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (decoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 1024)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 16)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerCrossAttention(\n",
       "              (EncDecAttention): T5Attention(\n",
       "                (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (2): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseActDense(\n",
       "                (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "                (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): ReLU()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       "  )\n",
       "  (adapter_to_t5): Linear(in_features=768, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2s_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ” Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 953/953 [08:40<00:00,  1.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "b2s_model.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for Bhist, Bpos in tqdm(test_data, desc=\"ðŸ” Evaluating\"):\n",
    "        try:\n",
    "            loss, pred_summary, gold_summary, _ = b2s_model(Bhist, Bpos, lookup_df, tail2idx, embed_tables)\n",
    "            true_tail = lookup_df.loc[Bpos, 'Tail']\n",
    "        except Exception as e:\n",
    "            print(f\"[Skip] {Bpos}: {e}\")\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            'Bpos': Bpos,\n",
    "            'true_tail_id': true_tail,\n",
    "            'generic summary':generic_summary,\n",
    "            'pred_summary': pred_summary,\n",
    "            'gold_summary': gold_summary\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"b2s_personalsum_eval_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
