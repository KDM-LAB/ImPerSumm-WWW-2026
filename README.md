# ImPerSum
**Information-Modulated Personalized Summarizer**

This repository contains the complete experimental pipeline for **ImPerSum**, including behavior graph construction, embedding preparation, dimensionality reduction, behavior encoding, and personalized summary generation with latent injection into T5.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 1. BEHAVIOR EXTRACTION & TRAINING INSTANCE GENERATION

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

### Input

CSV file with user interaction sequences.

Required columns:

â€¢ `UserID`   â†’ unique user identifier  
â€¢ `Docs`     â†’ stringified Python list of document IDs  
â€¢ `Action`   â†’ stringified Python list of actions aligned with `Docs`


### Processing Logic

â‘  **Behavior Graph Construction**  

â€¢ Each interaction is mapped to a unique `EdgeID` (B1, B2, â€¦)

User â”€â”€(action)â”€â”€â–¶ Docâ‚€
Docáµ¢â‚‹â‚ â”€â”€(action)â”€â”€â–¶ Docáµ¢


â‘¡ **Behavior Lookup Table**

EdgeID | Head | Relation | Tail | User | Dwell

â‘¢ **Dwell Time Augmentation**  

â€¢ `click` â†’ dwell from PENS dataset âˆˆ [20, 1230]  

â€¢ otherwise â†’ NaN

â‘£ **Training Instance Extraction**  
For every `summ_gen` action:

Bhist = all EdgeIDs before the event
Bpos = EdgeID of the current summ_gen


One training instance is created per `summ_gen`.

### Outputs
â€¢ `Behavior_Vocab.csv` â€” global behavior graph  
â€¢ `train_df` â€” supervision tuples `(Bhist â†’ Bpos)`

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 2. CORPUS â†’ EMBEDDING CONVERSION

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Dense semantic embeddings are generated for:

â€¢ News headlines  
â€¢ News bodies  
â€¢ Summaries  

Using:

â€¢ **E5 / T5 encoders**
â€¢ Mean pooling + L2 normalization
â€¢ Stored as `{ID â†’ embedding}` pickle dictionaries

These embeddings serve as **fixed semantic nodes** for all downstream modules.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 3. PCA DIMENSIONALITY REDUCTION (768 â†’ 192)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

To reduce memory and improve efficiency, all embeddings are projected to 192 dimensions.

### Key Properties

â€¢ **Incremental PCA** (shared across headline, body, summary embeddings)  
â€¢ Batch-wise fitting for scalability  
â€¢ Single PCA model reused across all modalities  

### Outputs 

headline_T5_pca192.pkl
newsbody_T5_pca192.pkl
summary_T5_pca192.pkl


Each preserves the original ID â†’ vector mapping with reduced dimensionality.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 4. BEHAVIOR ENCODER (Paper-Correct)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

The **BehaviorEncoder** implements the paperâ€™s formulation with:

â€¢ Action-specific gates  
â€¢ KDE-based mutual information estimation  
â€¢ Short-term, long-term, and event memory kernels  
â€¢ Adaptive Memory Fusion (AMF)  
â€¢ Tail-ID classification objective  

### Encoder Outputs

â€¢ Final user behavior state `z_b`  
â€¢ Predicted next-behavior embedding  
â€¢ Supervised next-tail prediction loss  

This encoder explicitly models **information modulation per action**, consistent with the ImPerSum formulation.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 5. PSEUDO-INVERSE SUMMARY NODE RECOVERY

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

A learned pseudo-inverse mapping recovers a latent **summary node embedding** from the user behavior state:
z_b â†’ Ãª_b â†’ Ãª_s


This enables summary-level personalization without direct text conditioning.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 6. PERSONALIZED GENERATION (B2S MODEL)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

The **B2SModel** integrates:

â€¢ BehaviorEncoder  
â€¢ Pseudo-Inverse Mapper  
â€¢ Cross-Attention (Eq. 19) between recovered summary node and document  
â€¢ Latent prefix injection into **T5-Large** decoder  

### Training Objective
Total Loss = 0.5 Ã— Behavior Encoding Loss + 0.5 Ã— T5 Generation Loss


### Generation
Personalized summaries are generated **autoregressively** using the injected latent prefix, without modifying the T5 architecture.

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“˜ 7. EVALUATION ARTIFACTS

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

The evaluation pipeline outputs:
Bpos
true_tail_id
generic summary
predicted personalized summary
gold summary

